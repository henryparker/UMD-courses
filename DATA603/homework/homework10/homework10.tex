
\documentclass[12pt]{article}
%\documentstyle[12pt]{article}
%\documentclass{amsart}
%\usepackage[dvips]{graphicx}


\usepackage{amssymb,amsmath,amscd,amsthm}
%\usepackage{graphicx,psfrag,epsfig,multirow} LINEA ORIGINAL
\usepackage{graphicx,psfrag,epsfig}


\usepackage{graphicx}
\usepackage{float}
\usepackage[active]{srcltx}

\usepackage{minted}
\usepackage{xcolor}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{conjecture}[theorem]{Conjecture}
\newtheorem{lemma}[theorem]{Lemma}
%\newtheorem{remark}[theorem]{Remark}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{example}[theorem]{Example}
\newtheorem{axiom}{Axiom}
\newtheorem{remark}{Remark}
\newtheorem{exercise}{Exercise}[section]

\newcommand{\thmref}[1]{Theorem~\ref{#1}}
\newcommand{\propref}[1]{Proposition~\ref{#1}}
\newcommand{\secref}[1]{\S\ref{#1}}
\newcommand{\lemref}[1]{Lemma~\ref{#1}}
\newcommand{\corref}[1]{Corollary~\ref{#1}}
\newcommand{\remref}[1]{Remark~\ref{#1}}



\setlength{\topmargin}{0mm}
\setlength{\oddsidemargin}{0mm}
\setlength{\textwidth}{160mm}
\setlength{\textheight}{215mm}
\font\bbc=msbm10 scaled 1200
\newcommand{\E}{\mathbf{E}}
\newcommand{\R}{\mbox {\bbc R}}
\newcommand{\T}{\mbox {\bbc T}}
\newcommand{\Z}{\mbox {\bbc Z}}
\def\stackunder#1#2{\mathrel{\mathop{#2}\limits_{#1}}}

\def\Area{{\rm Area}}
\def\Const{{\rm Const}}
\def\Int{{\rm Int}}

\def\eps{{\varepsilon}}

\def\EXP{\mathbb{E}}
\def\GR{\mathbb{G}}
\def\PROB{\mathbb{P}}
\def\TOR{\mathbb{T}}

\def\naturals{\mathbb{N}}

\def\brGamma{{\bar\Gamma}}
\def\brgamma{{\bar\gamma}}
\def\brtau{{\bar\tau}}
\def\brtheta{{\bar\theta}}
\def\brchi{{\bar\chi}}

\def\bI{{\bf I}}

\def\cE{\mathcal{E}}
\def\cG{\mathcal{C}}
\def\cL{\mathcal{L}}
\def\cU{\mathcal{U}}
\def\cZ{\mathcal{Z}}

\def\hN{{\hat N}}
\def\hn{{\hat n}}
\def\hy{{\hat y}}
\def\hGamma{{\hat\Gamma}}
\def\hdelta{{\hat\delta}}
\def\hsigma{{\hat\sigma}}
\def\htau{{\hat\tau}}
\def\heta{{\hat\eta}}
\def\htheta{{\hat\theta}}

\def\tW{{\tilde W}}
\def\tM{{\tilde M}}
\def\tX{{\tilde X}}
\def\tc{{\tilde c}}
\def\tp{{\tilde p}}
\def\tq{{\tilde q}}
\def\tdelta{{\tilde\delta}}
\def\teta{{\tilde\eta}}
\def\txi{{\tilde\xi}}
\def\tsigma{{\tilde\sigma}}
\def\ttheta{{\tilde\theta}}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\newminted{python}{
	linenos=true,
	bgcolor=backcolour,
	fontsize=\footnotesize,
	numbersep=5pt,
	frame=single,
	framesep=2mm
}

\title{Machine Learning Homework 10}
\author{Hairui Yin}
\date{}

\begin{document}
\maketitle
\noindent {\bf 1.}\\
\\
\textbf{(a)} We construct a simple autoencoder with a linear encoder and decoder as an inherit from Autoencoder from torch.nn.module. The data is standized using StandardScaler in sklearn. The model is trained with learning rate of 0.01, optimizer as Adam, loss function as MSE. And seed is fixed. As the code shows below:
\begin{pythoncode}
import torch
import torch.nn as nn
import torch.optim as optim
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
def seed_torch(seed):
	torch.manual_seed(seed)
	torch.cuda.manual_seed(seed)
	torch.cuda.manual_seed_all(seed)
	torch.backends.cudnn.deterministic = True
	torch.backends.cudnn.benchmark = False
seed_torch(1)

class Autoencoder(nn.Module):
	def __init__(self, input_dim, code_dim, activation=None):
		super(Autoencoder, self).__init__()
		self.encoder = nn.Linear(input_dim, code_dim, dtype=torch.float64)
		self.decoder = nn.Linear(code_dim, input_dim, dtype=torch.float64)
		self.activation = activation
	def forward(self, x):
		if self.activation:
			encoded = self.activation(self.encoder(x))
			decoded = self.decoder(encoded)
		else:
			encoded = self.encoder(x)
			decoded = self.decoder(encoded)
		return decoded

def train_autoencoder(model, data, epochs=1000, learning_rate=0.01):
	criterion = nn.MSELoss()
	optimizer = optim.Adam(model.parameters(), lr=learning_rate)
	for epoch in range(epochs):
		optimizer.zero_grad()
		reconstructed = model(data)
		loss = criterion(reconstructed, data)
		loss.backward()
		optimizer.step()
	return model, loss.item()
		
# Main
data = pd.read_csv('Pizza.csv')
features = data.iloc[:, 2:9].values
scalar = StandardScaler()
features = scalar.fit_transform(features)
X = torch.tensor(features, dtype=torch.float64).to(device)
input_dims, hid_dims = X.shape[1], range(1, 7)
# Linear
linear_mse = []
for d in hid_dims:
	model = Autoencoder(input_dims, d).to(device)
	trained_model, mse = train_autoencoder(model, X)
	pred_y = trained_model(X)
	mse = torch.mean((X - pred_y) ** 2).item()
	linear_mse.append(mse)
	
# Result
print(linear_acc)
\end{pythoncode}

MSE given by each hidden dimensions is
\begin{table}[h!]
	\begin{center}
		\caption{MSE and hidden size}
		\begin{tabular}{|c|c|c|c|c|c|c|}
			\hline
			h & 1 & 2 & 3 & 4 & 5 & 6\\
			\hline
			MSE & 0.4040 & 0.0768 & 0.0176 & 0.0040 & 0.0001 & 0.00006\\
			\hline
		\end{tabular}
	\end{center}
\end{table}
\newpage
\noindent \textbf{(b)} We add ReLU activation function before the hidden layer, and retrain the model. The code follows is:
\begin{pythoncode}
relu_mse = []
for d in hid_dims:
	relu_model = Autoencoder(input_dims, d, activation=nn.ReLU()).to(device)
	relu_model, mse = train_autoencoder(relu_model, X)
	reconstructed = relu_model(X)
	mse = torch.mean((X - reconstructed) ** 2).item()
	relu_mse.append(mse)
	
# Result
print(relu_mse)
\end{pythoncode}
Then we plot the MSE as a function of h (both linear and non-linear):
\begin{pythoncode}
plt.plot(hid_dims, linear_mse, label='Linear Autoencoder')
plt.plot(hid_dims, relu_mse, label='ReLU Autoencoder')
plt.xlabel('Code Dimension (h)')
plt.ylabel('Mean Squared Error (MSE)')
plt.title('MSE vs Code Dimension')
plt.legend()
plt.show()
\end{pythoncode}
\begin{figure}[H]
	\centering
	\includegraphics[scale=0.8] {q1.png}
\end{figure}
\newpage
\noindent {\bf 2.}\\
The optimal autoencoder in \textbf{q1} has hidden layer dimension $h=6$. Let the encoder weight matrix be $W$, decoder weight matrix be $V$, encoder bias be $W_b$ and decoder bias be $V_b$, we can get these item using code below:
\begin{pythoncode}
W, V = trained_model.encoder.weight.data, trained_model.decoder.weight.data
Wb, Vb = trained_model.encoder.bias, trained_model.decoder.bias
W, V, Wb, Vb
\end{pythoncode}
And output is:
\begin{figure}[H]
	\centering
	\includegraphics[scale=0.6] {q2.png}
\end{figure}
Consider shape of W is $[6,7]$, shape of Wb is $[6]$, shape of V is $[7, 6]$, shape of $V_b$ is $[7]$, and singular values of $X$ has shape $[7]$. The MSE value given input $x$ is calculated through:
$$MSE=||V(Wx+W_b)+V_b-x||_F^2$$
%\\

\end{document}
