
\documentclass[12pt]{article}
%\documentstyle[12pt]{article}
%\documentclass{amsart}
%\usepackage[dvips]{graphicx}


\usepackage{amssymb,amsmath,amscd,amsthm}
%\usepackage{graphicx,psfrag,epsfig,multirow} LINEA ORIGINAL
\usepackage{graphicx,psfrag,epsfig}


\usepackage{graphicx}
\usepackage{float}
\usepackage[active]{srcltx}

\usepackage{minted}
\usepackage{xcolor}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{conjecture}[theorem]{Conjecture}
\newtheorem{lemma}[theorem]{Lemma}
%\newtheorem{remark}[theorem]{Remark}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{example}[theorem]{Example}
\newtheorem{axiom}{Axiom}
\newtheorem{remark}{Remark}
\newtheorem{exercise}{Exercise}[section]

\newcommand{\thmref}[1]{Theorem~\ref{#1}}
\newcommand{\propref}[1]{Proposition~\ref{#1}}
\newcommand{\secref}[1]{\S\ref{#1}}
\newcommand{\lemref}[1]{Lemma~\ref{#1}}
\newcommand{\corref}[1]{Corollary~\ref{#1}}
\newcommand{\remref}[1]{Remark~\ref{#1}}



\setlength{\topmargin}{0mm}
\setlength{\oddsidemargin}{0mm}
\setlength{\textwidth}{160mm}
\setlength{\textheight}{215mm}
\font\bbc=msbm10 scaled 1200
\newcommand{\E}{\mathbf{E}}
\newcommand{\R}{\mbox {\bbc R}}
\newcommand{\T}{\mbox {\bbc T}}
\newcommand{\Z}{\mbox {\bbc Z}}
\def\stackunder#1#2{\mathrel{\mathop{#2}\limits_{#1}}}

\def\Area{{\rm Area}}
\def\Const{{\rm Const}}
\def\Int{{\rm Int}}

\def\eps{{\varepsilon}}

\def\EXP{\mathbb{E}}
\def\GR{\mathbb{G}}
\def\PROB{\mathbb{P}}
\def\TOR{\mathbb{T}}

\def\naturals{\mathbb{N}}

\def\brGamma{{\bar\Gamma}}
\def\brgamma{{\bar\gamma}}
\def\brtau{{\bar\tau}}
\def\brtheta{{\bar\theta}}
\def\brchi{{\bar\chi}}

\def\bI{{\bf I}}

\def\cE{\mathcal{E}}
\def\cG{\mathcal{C}}
\def\cL{\mathcal{L}}
\def\cU{\mathcal{U}}
\def\cZ{\mathcal{Z}}

\def\hN{{\hat N}}
\def\hn{{\hat n}}
\def\hy{{\hat y}}
\def\hGamma{{\hat\Gamma}}
\def\hdelta{{\hat\delta}}
\def\hsigma{{\hat\sigma}}
\def\htau{{\hat\tau}}
\def\heta{{\hat\eta}}
\def\htheta{{\hat\theta}}

\def\tW{{\tilde W}}
\def\tM{{\tilde M}}
\def\tX{{\tilde X}}
\def\tc{{\tilde c}}
\def\tp{{\tilde p}}
\def\tq{{\tilde q}}
\def\tdelta{{\tilde\delta}}
\def\teta{{\tilde\eta}}
\def\txi{{\tilde\xi}}
\def\tsigma{{\tilde\sigma}}
\def\ttheta{{\tilde\theta}}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\newminted{python}{
	linenos=true,
	bgcolor=backcolour,
	fontsize=\footnotesize,
	numbersep=5pt,
	frame=single,
	framesep=2mm
}

\title{Machine Learning Homework 8}
\author{Hairui Yin}
\date{}

\begin{document}
\maketitle
\noindent {\bf 1.}\\
\\
\textbf{(a)} In this part, I define all functions to be called, containing the activation function, initialize parameter function. forward function and backpropagation function. Given only one sample $x=[2, 1],y=3$, initialize all weights i.i.d uniform(0,1), then train the model 50 epoch with $\gamma=0.05$. Activation Function sigmoid, and loss is ssquared error.
\begin{pythoncode}
import numpy as np
import matplotlib.pyplot as plt

def sigmoid(z):
	return 1 / (1 + np.exp(-z))
def sigmoid_derivative(z):
	return (sigmoid(z) * (1 - sigmoid(z)))

def initialize_parameter(input_size, hidden_size, output_size, random=42):
	np.random.seed(random)
	# include bias
	theta1 = np.random.uniform(0, 1, (input_size + 1, hidden_size))
	theta2 = np.random.uniform(0, 1, (hidden_size + 1, output_size))
	return theta1, theta2

def forward(x, theta1, theta2):
	a1 = x
	z2 = np.concatenate((np.array([1]), a1)) @ theta1
	a2 = sigmoid(z2)
	z3 = np.concatenate((np.array([1]), a2)) @ theta2
	return z2, a2, z3

def backward(x, y, z2, a2, z3, theta1, theta2):
	dz3 = z3 - y
	# Gradient for theta2
	dtheta2 = (dz3 * np.concatenate((np.array([1]), a2))).reshape([3,1])
	# Gradient for theta1
	da2 = dz3 * theta2[1:]
	dz2 = da2 * sigmoid_derivative(z2).reshape(2,1)
	dtheta1 = np.outer(dz2, np.concatenate((np.array([1]), x))).reshape([3,2])
	
	return dtheta1, dtheta2
\end{pythoncode}
\begin{pythoncode}
theta1, theta2 = initialize_parameter(2, 2, 1)
x, y = np.array([2, 1]), np.array([3])
learning_rate = 0.05

iterations = []
loss = []
for i in range(50):
	z2, a2, z3 = forward(x, theta1, theta2)
	dtheta1, dtheta2 = backward(x, y, z2, a2, z3, theta1, theta2)
	theta1 -= learning_rate * dtheta1
	theta2 -= learning_rate * dtheta2
	# loss record
	iterations.append(i)
	loss.append((y-z3)**2)
\end{pythoncode}
\textbf{(b)} Since we have recorded squared error loss in question (a), we use the following code to display it:
\begin{pythoncode}
plt.figure(figsize=(8, 5))
plt.plot(iterations, loss, marker='o', linestyle='-', color='b', label="Loss")
plt.title("Loss vs Iterations")
plt.xlabel("Iterations")
plt.ylabel("Loss")
plt.legend()
plt.grid()
plt.show()
\end{pythoncode}
And the figure is:
\begin{figure}[H]
	\centering
	\includegraphics[scale=0.8] {figure1.png}
\end{figure}
%\\
%\\

\end{document}
